# -*- coding: utf-8 -*-
"""Loan risk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gda1oeM1NMdGvxczMv5VVqMivx21gMqz
"""


import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import keras_tuner as kt
from IPython.core.interactiveshell import InteractiveShell

InteractiveShell.ast_node_interactivity = "all"

path = "/content/Loan-Risk/dataset/test.csv"
df = pd.read_csv(path)
df.head(7)

df.iloc[8726,8] = np.nan

"""# Data Visualtization

"""

for i in df.columns:
    print(i,str(df.shape[0] - df[i].notnull().sum()))

# print(df.occupation_type.unique())
# for i in df.columns:
#     if(df[i].isnull().sum() != 0):
#         print(i)
#         print(((df[df[i].isnull()]['credit_card_default'].sum())/df[i].isnull().sum())*100)

df.hist(figsize = (40,30), bins = 100)

ax = sns.boxplot(data = df.yearly_debt_payments,orient = 'v')
ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)

"""## Fill NaN values and outlier detection"""

df[df['credit_limit']> 5000000]

a = [3,4,5,9]
df.owns_car.fillna('NaN',inplace = True)
for i in a:
    df.iloc[:,i] = pd.Categorical(df.iloc[:,i])
    df[df.columns[i]] = df[df.columns[i]].cat.codes

# df.credit_card_default = pd.Categorical(df.credit_card_default)
# df.credit_card_default = df.credit_card_default.cat.codes
# df.owns_car.unique()
# df
# df.mean()
# df.credit_card_default

df.no_of_children.fillna(0,inplace = True)
# change this to zero and see the difference
aa = df[df.no_of_days_employed.isnull()]
df.no_of_days_employed.iloc[aa.index] = (df.iloc[aa.index].age - 18)*365

df.total_family_members.fillna(2,inplace = True)
df.migrant_worker.fillna(0,inplace = True)
df.yearly_debt_payments.fillna(df.yearly_debt_payments.mean(),inplace = True)
df.credit_score.fillna(df.credit_score.mean(),inplace = True)

# ss = ((df.age[df.no_of_days_employed < df.age*365] - round(df.no_of_days_employed[df.no_of_days_employed < df.age*365]/365))<5)
# ss
# aa = df.no_of_days_employed.iloc[ss[ss == True].index]
xx = df[df.no_of_days_employed > df.age*365]
xx
temp = df.copy()
# # temp.no_of_days_employed.iloc[aa.index] = (temp.age.iloc[aa.index]*365) - (18*365)
temp.no_of_days_employed.iloc[xx.index] = (temp.age.iloc[xx.index] - 18)*365
temp.no_of_days_employed.iloc[xx.index]

"""# Change temp to df

"""

a1 = df
df = temp
# df = a1
# df.hist(figsize = (40,20))

"""# Create new features

"""

a = df.owns_car == 1
b = df.owns_house == 1
c = a&b

df.insert(loc = 6, value = c, column = 'owns_car_and_house')

df.owns_car_and_house = pd.Categorical(df.owns_car_and_house)
df.owns_car_and_house = df.owns_car_and_house.cat.codes

a = df.migrant_worker == 1.0
b = df.owns_house == 1
c = a&b
df.insert(loc = 10, value = c, column = 'owns_house_and_migrant')
df.owns_house_and_migrant = pd.Categorical(df.owns_house_and_migrant)
df.owns_house_and_migrant = df.owns_house_and_migrant.cat.codes
df

c = df.yearly_debt_payments * df.credit_limit
df.insert(loc = 11, value = c, column = 'Product')

df.Product[df.Product < 10000000000].hist()

"""# Normalization and Skew Check"""

df['net_yearly_income'] = np.log1p(df['net_yearly_income'])
df['no_of_days_employed'] = np.log1p(df['no_of_days_employed'])
df['yearly_debt_payments'] = np.log1p(df['yearly_debt_payments'])
df['credit_limit'] = np.log1p(df['credit_limit'])
df['Product'] = np.log1p(df['Product'])

df.credit_score = df.credit_score/100

df.hist(figsize = (40,40), bins = 200)

# df[df.net_yearly_income < 10000000].hist(figsize = (40,40), bins = 200)

"""# Isolation and outlier detection

"""

import sklearn as sklearn
from sklearn import ensemble

# contamination =((df.credit_card_default == 1).sum())/((df.credit_card_default == 0).sum())
# contamination
model = sklearn.ensemble.IsolationForest(n_estimators = 5000,contamination = 'auto' ,max_features = 10)

X = df.iloc[:,2:]
Y = model.fit_predict(X)

Y[Y == -1].shape

isolated_df = df[Y==-1]
unisolated_df = df[Y==1]
# (isolated_df.credit_card_default == 1).sum()/(df.credit_card_default == 1).sum()
# print((isolated_df['credit_card_default'] == 1).sum())

"""# Model for the isolated Dataframe

"""

import xgboost as xgb

X = isolated_df.iloc[:,2:-1]
X
Y = isolated_df.iloc[:,-1]
Y
D_df = xgb.DMatrix(X,label = Y)

classifier = xgb.XGBClassifier(max_depth = 10, n_estimators = 500)
classifier.fit(X,Y,eval_metric='logloss',verbose = True)
pred = classifier.predict(X)

sklearn.metrics.confusion_matrix(pred,Y)
iso_prediction_df = isolated_df.copy()
iso_prediction_df.iloc[:,-1] = pred

import pickle
file_name = "classifier1.pkl"

# save
pickle.dump(classifier, open(file_name, "wb"))

iso_prediction_df

"""# Unisolated Dataframe
1. Sampling data
"""

X = unisolated_df.iloc[:,2:-1]
Y = unisolated_df.iloc[:,-1]
from imblearn.over_sampling import SMOTE, ADASYN
X_resampled, Y_resampled = SMOTE().fit_resample(X, Y)

"""2. train model

"""

import six
import sys
sys.modules['sklearn.externals.six'] = six

from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier
from mlxtend.classifier import StackingCVClassifier

model1 = MLPClassifier(activation = "relu", alpha = 0.1, hidden_layer_sizes = (10,10,10),
                            learning_rate = "constant", max_iter = 2000)


# Initializing Random Forest classifier
model2 = RandomForestClassifier(n_estimators = 500, criterion = "gini", max_depth = 10,
                                     max_features = "auto")
model3 = AdaBoostClassifier(n_estimators = 500)
model4 = BaggingClassifier(n_estimators = 40)
model5 = xgb.XGBClassifier(max_depth = 10, n_estimators = 500)

sclf = StackingCVClassifier(classifiers = [ model2, model3, model4,model5],
                            shuffle = False,
                            use_probas = True,
                            cv = 5,
                            meta_classifier = model5)

classifiers = {"MLP": model1,
               "RF": model2,
               "ADA": model3,
               "BAG": model4,
               "XGB": model5,
               "Stack": sclf}

sclf.fit(X_resampled.values, Y_resampled.values)
# for key in classifiers:
#     # Get classifier
#     classifier = classifiers[key]
    
#     # Fit classifier
#     classifier.fit(X, Y)
        
#     # Save fitted classifier
#     classifiers[key] = classifier

classifier2 = xgb.XGBClassifier(max_depth = 12, n_estimators = 500)
classifier2.fit(X_resampled,Y_resampled,eval_metric='logloss',verbose = True)
# classifier2 = sclf

pred = classifier2.predict(X_resampled)
sklearn.metrics.confusion_matrix(pred,Y_resampled)

file_name = "classifier2.pkl"

# save
pickle.dump(classifier2, open(file_name, "wb"))

"""# Test Data

"""

classifier1 = pickle.load(open("classifier1.pkl", "rb"))
classifier2 = pickle.load(open("classifier2.pkl","rb"))

isolated_df

pred1 = classifier1.predict(isolated_df.iloc[:,2:])
pred2 = classifier2.predict(unisolated_df.iloc[:,2:])
copy1 = isolated_df.copy()
copy2 = unisolated_df.copy()

copy1['credit_card_default'] = pred1
copy2['credit_card_default'] = pred2
copy1.head(7)
copy2.head(7)

newdf = pd.concat([copy1,copy2],axis = 0)
newdf = newdf.sort_index()

newdf.to_csv('aman2.csv')

submission =

def build_model(hp):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Flatten())
    a = hp.Int('num_layers', 2, 5)
    print("************************ ",a)
    for i in range(a):
        print("%%%%%%%%%%%%%%%%%%%%%%%  i")
        model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu',
                               kernel_regularizer = tf.keras.regularizers.l2()))
        
        
    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss=tf.keras.losses.binary_crossentropy,
        metrics=tf.keras.metrics.BinaryAccuracy())
    return model

tuner = kt.BayesianOptimization(
    build_model,
    objective=kt.Objective("val_binary_accuracy", direction="max"),
    max_trials=5,
    executions_per_trial=3
    )

X_train = df.iloc[:,2:-1]
Y_train = df.iloc[:,-1]
print([i for i in range(2)])

from sklearn.model_selection import train_test_split
X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.3, random_state=0)
X_train = np.asarray(X_train).astype(np.int)
Y_train = np.asarray(Y_train).astype(np.int)
X_val = np.asarray(X_val).astype(np.int)
Y_val = np.asarray(Y_val).astype(np.int)

tuner.search(X_train, Y_train,
             epochs=5,
             validation_data=(X_val, Y_val))

model = tuner.get_best_models(num_models = 1)
model = model[0]
model.build(input_shape = (None, 16))

model.summary()
model.fit(x = X_train, y = Y_train, batch_size = 32, epochs = 100, validation_data = (X_val,Y_val))

df = pd.read_csv('/content/Loan-Risk/dataset/test.csv')
a = [3,4,5,9]
df.owns_car.fillna('N',inplace = True)
for i in a:
    df.iloc[:,i] = pd.Categorical(df.iloc[:,i])
    df[df.columns[i]] = df[df.columns[i]].cat.codes



df.no_of_children.fillna(0,inplace = True)
#change this to zero and see the difference
df.no_of_days_employed.fillna(df.no_of_days_employed.mean(),inplace = True)
df.total_family_members.fillna(2,inplace = True)
df.migrant_worker.fillna(0,inplace = True)
df.yearly_debt_payments.fillna(df.yearly_debt_payments.mean(),inplace = True)
df.credit_score.fillna(df.credit_score.mean(),inplace = True)
X_train = df.iloc[:,2:]

X_train

a = model.predict(X_train)
b = pd.DataFrame(a)
b = round(b)

k = pd.DataFrame(df.customer_id)
b = k.join(b)

b = b.rename(columns={0:"credit_card_default"})

b.to_csv('aman2.csv')

a = 0.4
round(a)

b

